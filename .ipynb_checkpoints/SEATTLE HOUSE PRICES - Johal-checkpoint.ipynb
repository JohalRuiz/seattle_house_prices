{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOSTON HOUSE PRICES - IRONHACK COMPETITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES:\n",
    "1. Call your transformer file 'tranformer.pkl', your encoder file 'encoder.pkl' and your model 'model.pkl'.\n",
    "2. This is just the skeleton, you'll need to adjust it accordingly. Watch out! Check if the order of transformed data and encoded data is correct, you don't want to concatenate stuff out of order.\n",
    "3. Test your function on a couple of rows to make sure everything runs fine and that you have the same predictions when you test it in your original notebook.\n",
    "4. Deliver a folder with your name and, inside the folder, this notebook with your changes and pickle files with model, transformer (if you have it) and encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your code Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(data):\n",
    "    \"\"\"\n",
    "    This function will take in a pandas dataframe,\n",
    "    apply cleaning and return the cleaned data.\n",
    "    \"\"\"\n",
    "    # your cleaning steps here #\n",
    "    data.columns = [column.replace(' ', '') for column in data.columns]\n",
    "    data.drop(columns=['id', 'index'], inplace=True)\n",
    "    data['data']=data['data'].astype('datetime64')\n",
    "    data['waterfront'] = data['waterfront'].astype('object')\n",
    "    data['view'] = data['view'].astype('object')\n",
    "    data['zip_code'] = data['zip_code'].astype('object')\n",
    "    data.drop(columns=['lon','lat', 'data', 'condition'], inplace=True)\n",
    "    # Condition is dropped because it's redundant with 'grade'\n",
    "    print(data.columns)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thre rest is pretty much done, but might need some twiking from your side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformer(data):\n",
    "    \"\"\"\n",
    "    This function will take in cleaned data, already with\n",
    "    the correct types and apply the transformations.\n",
    "    \"\"\"\n",
    "    # if you did not scale your y, it's likely that the split can be done here\n",
    "    X = data.drop('price', axis=1) # assuming you named your target variable as 'price'\n",
    "    y = data['price']\n",
    "    \n",
    "    numericals = X.select_dtypes(np.number).to_numpy()\n",
    "    categoricals = X.select_dtypes(np.object)\n",
    "    \n",
    "    num_col_names = X.select_dtypes(np.number).columns\n",
    "    cat_col_names = X.select_dtypes(np.object).columns\n",
    "    \n",
    "    try:\n",
    "        with open('transformer.pkl', 'rb') as file: \n",
    "            transformer = pickle.load(file)\n",
    "            \n",
    "        numericals = transformer.transform(numericals)\n",
    "    \n",
    "    except:\n",
    "        print(\"Transformation didn't work or transformer was not found!\")\n",
    "\n",
    "    try:\n",
    "        with open('encoder.pkl', 'rb') as file: \n",
    "            encoder = pickle.load(file)\n",
    "        \n",
    "        categoricals = encoder.transform(categoricals)\n",
    "    \n",
    "    except:\n",
    "        print(\"Encoding didn't work or encoder was not found!\")\n",
    "        \n",
    "    # make sure the concatenation is in the same order as the one you used to train your model\n",
    "    X_transformed = np.concatenate([numericals, categoricals], axis=1)\n",
    "    \n",
    "    X_transformed = pd.DataFrame(X_transformed)\n",
    "    X_transformed.columns = num_col_names.append(cat_col_names)\n",
    "    \n",
    "    ###\n",
    "    # if you have scaled y, you will do the X-y split here:\n",
    "    ###\n",
    "    return X_transformed, y\n",
    "\n",
    "\n",
    "def validation(X, y):\n",
    "    \n",
    "    with open('model.pkl', 'rb') as file: \n",
    "        model = pickle.load(file) \n",
    "        \n",
    "    try:\n",
    "        y_pred = model.predict(X)\n",
    "    except:\n",
    "        y_pred = model.predict(X.to_numpy())\n",
    "    \n",
    "    try:\n",
    "        mae = round(mean_absolute_error(y, y_pred), 2)\n",
    "        rmse = round(mean_squared_error(y, y_pred, squared=False), 2)\n",
    "        r2 = round(r2_score(y, y_pred), 2)\n",
    "    except:\n",
    "        mae = round(mean_absolute_error(y.to_array(), y_pred), 2)\n",
    "        rmse = round(mean_squared_error(y.to_array(), y_pred, squared=False), 2)\n",
    "        r2 = round(r2_score(y.to_array(), y_pred), 2)\n",
    "    \n",
    "    print('--------------')\n",
    "    print('MAE:', mae)\n",
    "    print('RMSE:', rmse)\n",
    "    print('R2 Score:', r2)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def get_predictions(data):\n",
    "    cleaned_data = data_cleaner(data)\n",
    "    transformed_data, y = data_transformer(cleaned_data)\n",
    "    predictions = validation(transformed_data, y)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('train_boston.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
      "       'waterfront', 'view', 'grade', 'sqft_above', 'sqft_basement',\n",
      "       'yr_built', 'yr_renovated', 'zip_code', 'sqft_living15', 'sqft_lot15',\n",
      "       'price'],\n",
      "      dtype='object')\n",
      "Transformation didn't work or transformer was not found!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 0 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d77206be7669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-9dc723b5c4fc>\u001b[0m in \u001b[0;36mget_predictions\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mcleaned_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mtransformed_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_transformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9dc723b5c4fc>\u001b[0m in \u001b[0;36mdata_transformer\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# make sure the concatenation is in the same order as the one you used to train your model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mX_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumericals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategoricals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mX_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 0 dimension(s)"
     ]
    }
   ],
   "source": [
    "predictions = get_predictions(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
